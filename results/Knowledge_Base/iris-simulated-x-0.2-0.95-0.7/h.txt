{
  "dataset": "iris-simulated-x",
  "data_folder_path": "/Users/kaizer/Documents/Active Learning/Code/MAAL/Multi-Annotator-HIL/datasets",
  "budget_frac": 0.2,
  "boot_size": 0.05,
  "test_size": 0.4,
  "boot_lr": 0.01,
  "boot_n_epochs": 3000,
  "boot_log_epochs": 30000,
  "boot_batchsize": 4,
  "active_lr": 0.02,
  "active_n_epochs": 2000,
  "active_log_epochs": 10000,
  "active_batchsize": 16,
  "hidden_dim": 32,
  "instance_strategy": "random",
  "method": "KB",
  "LR_max_iter": 20000,
  "classifier_name": "logistic_regression",
  "f1_average": "macro",
  "log_dir": "logs/iris-simulated-x/",
  "seed": 0,
  "labeling_type": "max",
  "exp_name": "Trial_1",
  "use_Knowledge_Base": true,
  "similarity_threshold": 0.95,
  "weight_threshold": 0.7,
  "data_source": "mapal",
  "w_opt": 1,
  "rl_flag": 0,
  "ee_ratio": 0.8,
  "Data_path": "/Users/kaizer/Documents/Active Learning/Code/MAAL/Multi-Annotator-HIL/datasets/iris-simulated-x.csv",
  "Path_results": "results/Knowledge_Base/iris-simulated-x-0.2-0.95-0.7",
  "Path_mapal_data": "data_processing/iris-simulated-x-0.2",
  "exp_txt_path": "results/Knowledge_Base/iris-simulated-x-0.2-0.95-0.7/comparison.txt"
}

MAPAL Accuracy : 0.6666666666666667



Number of classes : 3



Fully Supervised
0.96666666666666670.9674603174603175


Total number of Active Learning Instances : 25



Extra instances added due to knowledge base similarity : 16



KNOWLEDGE BASE METRICS

   No. of Instances  Accuracy  f1-score
0                 1       1.0       1.0
2                 2       1.0       1.0
3                19       1.0       1.0


SIMILAR INSTANCES METRICS

    No. of similar Instances Similar Instances  Similar Instances Shared Label  Accuracy  f1-score  Annotator
82                         1              [64]                               1       0.0  0.000000          2
93                         2          [81, 71]                               1       0.5  0.333333          3
7                          1              [23]                               0       1.0  1.000000          3
57                         1              [88]                               1       1.0  1.000000          3
88                         2          [61, 96]                               1       1.0  1.000000          3
81                         1              [67]                               1       0.0  0.000000          3
71                         1              [65]                               0       1.0  1.000000          3
74                         1              [50]                               2       1.0  1.000000          3
97                         1              [86]                               1       1.0  1.000000          3
86                         1              [52]                               1       1.0  1.000000          3
52                         1              [76]                               1       1.0  1.000000          3
61                         1             [110]                               1       1.0  1.000000          3
76                         2          [62, 87]                               1       0.5  0.333333          3


ANNOTATOR MODEL PREDICTED LABELS VS TRUE LABELS
Accuracy : 0.64
F1 Score : 0.5376623376623377
Confusion Matrix[[ 3  4  0]
 [ 0 12  1]
 [ 0  4  1]]
Classification Report              precision    recall  f1-score   support

           0       1.00      0.43      0.60         7
           1       0.60      0.92      0.73        13
           2       0.50      0.20      0.29         5

    accuracy                           0.64        25
   macro avg       0.70      0.52      0.54        25
weighted avg       0.69      0.64      0.60        25


W OPTIMAL LABELS VS TRUE LABELS
Accuracy : 0.64
F1 Score : 0.5664983164983165
Confusion Matrix[[ 3  2  0]
 [ 0 12  1]
 [ 0  6  1]]
Classification Report              precision    recall  f1-score   support

           0       1.00      0.60      0.75         5
           1       0.60      0.92      0.73        13
           2       0.50      0.14      0.22         7

    accuracy                           0.64        25
   macro avg       0.70      0.56      0.57        25
weighted avg       0.65      0.64      0.59        25


MAJORITY LABELS VS TRUE LABELS
Accuracy : 0.8
F1 Score : 0.7105105105105105
Confusion Matrix[[ 3  4  0]
 [ 0 16  1]
 [ 0  0  1]]
Classification Report              precision    recall  f1-score   support

           0       1.00      0.43      0.60         7
           1       0.80      0.94      0.86        17
           2       0.50      1.00      0.67         1

    accuracy                           0.80        25
   macro avg       0.77      0.79      0.71        25
weighted avg       0.84      0.80      0.78        25


Compostion of instance and number of queried annotators
[0, 1, 3, 5]
[65, 3, 1, 17]


CLASSIFIER METRICS

                 After Warmup Accuracy on Boot Data  After Warmup F1 Score on Boot Data  After Warmup Accuracy on Validation Data  After Warmup F1 Score on Validation Data  After Training Accuracy on Validation Data  After Training F1 Score on Validation Data
Annotator Model                                1.00                            1.000000                                  0.783333                                  0.765105                                    0.666667                                    0.559140
W Optimal                                      0.75                            0.555556                                  0.650000                                  0.551913                                    0.650000                                    0.551913
Majority                                       1.00                            1.000000                                  0.783333                                  0.765105                                    0.650000                                    0.543366
True Labels                                    1.00                            1.000000                                  0.783333                                  0.765105                                    0.783333                                    0.760718


ANNOTATOR METRIC

                  Annotator Accuracy after Warmup on Boot Data  Annotator F1 Score after Warmup on Boot Data  Annotator Accuracy after Training  Annotator F1 Score after Training
Weighted Average                                          0.75                                           0.6                               0.72                           0.600000
Maximum Index                                             1.00                                           1.0                               0.72                           0.615152