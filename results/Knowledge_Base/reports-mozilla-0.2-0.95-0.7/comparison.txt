{
  "dataset": "reports-mozilla",
  "data_folder_path": "datasets",
  "budget_frac": 0.2,
  "boot_size": 0.05,
  "test_size": 0.4,
  "boot_lr": 0.001,
  "boot_n_epochs": 100,
  "boot_log_epochs": 30000,
  "boot_batchsize": 4,
  "active_lr": 0.001,
  "active_n_epochs": 100,
  "active_log_epochs": 20000,
  "active_batchsize": 16,
  "hidden_dim": 32,
  "instance_strategy": "random",
  "method": "KB",
  "LR_max_iter": 10000,
  "classifier_name": "logistic_regression",
  "f1_average": "macro",
  "log_dir": "logs/iris-simulated-x/",
  "seed": 1,
  "labeling_type": "max",
  "exp_name": "Trial_1",
  "use_Knowledge_Base": true,
  "similarity_threshold": 0.95,
  "weight_threshold": 0.7,
  "data_source": "mapal",
  "w_opt": 1,
  "rl_flag": 0,
  "ee_ratio": 0.8,
  "Data_path": "datasets/reports-mozilla.csv",
  "Path_results": "results/Knowledge_Base/reports-mozilla-0.2-0.95-0.7",
  "Path_mapal_data": "data_processing\\reports-mozilla-0.2",
  "exp_txt_path": "results/Knowledge_Base/reports-mozilla-0.2-0.95-0.7/comparison.txt"
}

MAPAL Accuracy : 0.6296296296296298



Number of classes : 4



Fully Supervised

Accuracy : 0.6296296296296297
F1 Score : 0.5633785322642673


KNOWLEDGE BASE METRICS

   No. of Instances  Accuracy  f1-score
1                 6  0.833333      0.70
3                 3  0.333333      0.25
0                 5  1.000000      1.00
4                 6  1.000000      1.00


SIMILAR INSTANCES METRICS

Empty DataFrame
Columns: []
Index: []


Total number of Active Learning Instances : 244



Extra instances added due to knowledge base similarity : 1



KNOWLEDGE BASE METRICS

   No. of Instances  Accuracy  f1-score
1                67       1.0       1.0
3                17       1.0       1.0
0                35       1.0       1.0
4                63       1.0       1.0
2                36       1.0       1.0


SIMILAR INSTANCES METRICS

     No. of similar Instances Similar Instances  Similar Instances Shared Label  Accuracy  f1-score  Annotator
544                         1             [542]                               1       1.0       1.0          1


ANNOTATOR MODEL PREDICTED LABELS VS TRUE LABELS
Accuracy : 0.7868852459016393
F1 Score : 0.7480902567711107
Confusion Matrix[[54  1  0  3]
 [ 6 16  6  2]
 [ 3  8 78  6]
 [ 5  4  8 44]]
Classification Report              precision    recall  f1-score   support

           0       0.79      0.93      0.86        58
           1       0.55      0.53      0.54        30
           2       0.85      0.82      0.83        95
           3       0.80      0.72      0.76        61

    accuracy                           0.79       244
   macro avg       0.75      0.75      0.75       244
weighted avg       0.79      0.79      0.78       244


W OPTIMAL LABELS VS TRUE LABELS
Accuracy : 0.7459016393442623
F1 Score : 0.7063758405020979
Confusion Matrix[[56  3  1  2]
 [ 5 14  8  3]
 [ 5 11 73 11]
 [ 2  1 10 39]]
Classification Report              precision    recall  f1-score   support

           0       0.82      0.90      0.86        62
           1       0.48      0.47      0.47        30
           2       0.79      0.73      0.76       100
           3       0.71      0.75      0.73        52

    accuracy                           0.75       244
   macro avg       0.70      0.71      0.71       244
weighted avg       0.74      0.75      0.74       244


MAJORITY LABELS VS TRUE LABELS
Accuracy : 0.8401639344262295
F1 Score : 0.823742135272966
Confusion Matrix[[60  0  1  3]
 [ 4 25  9  3]
 [ 2  4 76  5]
 [ 2  0  6 44]]
Classification Report              precision    recall  f1-score   support

           0       0.88      0.94      0.91        64
           1       0.86      0.61      0.71        41
           2       0.83      0.87      0.85        87
           3       0.80      0.85      0.82        52

    accuracy                           0.84       244
   macro avg       0.84      0.82      0.82       244
weighted avg       0.84      0.84      0.84       244


Compostion of instance and number of queried annotators
[0, 1, 2, 3, 4, 5]
[161, 136, 7, 10, 6, 65]


CLASSIFIER METRICS

                 After Warmup Accuracy on Boot Data  After Warmup F1 Score on Boot Data  After Warmup Accuracy on Validation Data  After Warmup F1 Score on Validation Data  After Training Accuracy on Validation Data  After Training F1 Score on Validation Data
Annotator Model                                0.85                            0.664530                                  0.548148                                  0.414269                                    0.688889                                    0.578709
W Optimal                                      0.85                            0.682749                                  0.596296                                  0.415695                                    0.629630                                    0.481473
Majority                                       1.00                            1.000000                                  0.625926                                  0.499801                                    0.644444                                    0.553704
True Labels                                    1.00                            1.000000                                  0.625926                                  0.499801                                    0.629630                                    0.559241


ANNOTATOR METRIC

                  Annotator Accuracy after Warmup on Boot Data  Annotator F1 Score after Warmup on Boot Data  Annotator Accuracy after Training  Annotator F1 Score after Training
Weighted Average                                          0.80                                      0.629808                            0.72541                           0.690563
Maximum Index                                             0.85                                      0.664530                            0.75000                           0.713637